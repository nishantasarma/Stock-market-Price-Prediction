include(Screenshot of datasets)#google finance 1 page
inlcude(Screenshot of dataset)% google about info 2 page
---------------------------------------------------------------------------------------------------------------------------


After getting the data from various sources, the entire project can be broken down into the following sub steps
1.Data gathering
2. preprocessingg
3. feature extraction
4. normalization of data
5. Modelling
6. Error calculation

----------------------------------------------------------------------------------------------------------------------------

Preprocessing ---- In this step the data obtained is cleaned that is 

1.Sort the data according to the date

2.some of the data from the dataset contains missing values which must filled for this we resample the data taking samples for each day and interpolate(linear interpolation).

3.Concatentae the dispartate datasets into one single dataset.

The screenshot of the output of these two procees is shown in the next few pages
----------------------------------------------------------------------------------------------------------------------------
feature extraction

We extract the following features 


	1.Moving average of the closing price
	2.Momentum per day of the closing price and volume
	screenshots to be included

----------------------------------------------------------------------------------------------------------------------------
Splittting the data into training and testing sets

We split the data into trainning and testing sets.We keep 80% of the data for training and the rest we keep for testing.
Screeenshot

----------------------------------------------------------------------------------------------------------------------------

Normalization

Normaliization of data is important for the model to reach convergence quickly.  Without normalization the data may even never converge to the minimum error.
THe model might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.
In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.

Also  if a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.

We calculate the mean and standard deviation for the training set and later reapply the same mean and standard deviatiion in the testing set.
Formuala for normalization used = X-mean / standard Deviation
---------------------------------------------------------------------------------------------------------------------------

Modelling
We have used Support Vector regression algorithm to model our data as we had only 30 days of data and SVR works quite well
for lower number of training examples and we have used the kernel radial basis function(Rbf). 

include a self suddicient introduction to SVR for beginers #3-4 pages 

----------------------------------------------------------------------------------------------------------------------------

Error calculation---
For this we simply take our predicted data subtact it from the testing set and take the mean.
Y(predicted)-Y/number of samples.

----------------------------------------------------------------------------------------------------------------------------
Screenshots of the code
----------------------------------------------------------------------------------------------------------------------------

Furure work








